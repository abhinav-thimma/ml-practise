{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting where the decision trees stopped,\n",
    "- we know that decision trees are very easy to build and comprehend\n",
    "- they provide quick predictions\n",
    "- but, they do not perform great on unseen data \n",
    "- Random Forest is specifically here to combine the simplicity of the decision trees and reduce their inaccuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to go about with a random forest classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to follow the below steps:\n",
    "1. we must create several bootstrapped datasets. (this meand we need to randomly select different rows from our data into new datasets. These rows could also be repeated)\n",
    "2. Once we have the random bootstrapped datasets, we select random subset of the features or columns for each iteration instead of considering all the remaining columns.\n",
    "3. Repeat step 1 and 2, several times to build many trees\n",
    "4. Pass the new datapoint through all the built trees, and find individual results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post this, we can aggregate all the results and provide a common outcome from the algorithms. This step is called bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we measure how good our random forest is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we are randomly selecting rows for building the bootstrapped dataset, we end up with some rows not making the final dataset. These are called \"Out of bag dataset\"\n",
    "- Once we identify these out of bag samples, we can run them through all our trees and aggregate their results to check if the entire model is predicting accurately\n",
    "- Ultimately, the accuracy of our random forest depends on the proportion of out of bag samples correctly classified by our model\n",
    "- The proportion of incorrectly classified samples in out of bags dataset is called as out of bag error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to further imporve the random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we now about the \"out of bag error\", we can vary the number of randomly selected features we choose while building our random forest and find which has the lowest out of bag error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we handle missing data in Random Forests? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most common approach taken to fill in the missing values is to fill in some intial values and then refine the values as we go ahead\n",
    "- A common approach taken is to find the most common value for a mising column based on the output\n",
    "\n",
    "Eg: if heart disease is the output boolean and we have a missing value in blocked artery, we find the value of heart disease for this entry, lets say it is true.\n",
    "So, we find all the output values which are true and find the most frequest value in the column of blocked artery.\n",
    "- Similarly we can assume the median value if we have numberic columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that we started this process by filling in missing values as stated before, How can we actually replace these values with better estimates?\n",
    "\n",
    "- we need to build the Random Forest trees similar to the normal process with the filled data\n",
    "- Then we run each entry through each tree\n",
    "- while doing this process we record a promximity matrix. This matrix would indicate that two entries are similar if they end up on the same leaf node\n",
    "- Cumulate the results in the proximity matrix by running all records through all the trees\n",
    "- Divide all the values in the proximity matrix with the number of trees in the random forest\n",
    "\n",
    "After this process we ned to find the weighted frequencies as explained in the following video:\n",
    "- https://www.youtube.com/watch?v=sQ870aTKqiM\n",
    "- Post this we can accurately estimate what the missing values could have been\n",
    "\n",
    "Similarly we can also estimate the numeric values using weighted average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The above process is repeated several times until the missing values converge or do not change much in successive runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The uses of proximity matrices:\n",
    "- proximity matrices can be used to compute the distance matrix which can further be used in plotting heat maps and MDS plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data in new data for which we want to predict the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we create copies of the data with all possible values for the result\n",
    "- for each of these copies we run through the above method and fill in the missing values\n",
    "- once filled in, we drop the result values and normally run through our random forest\n",
    "- we verify which version of our missing values produced same result as filled in value most times and use that as our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source:\n",
    "- https://www.youtube.com/watch?v=J4Wdy0Wc_xQ\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html#bagging\n",
    "- https://www.youtube.com/watch?v=sQ870aTKqiM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees Classifier:  https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
