{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting where the decision trees stopped,\n",
    "- we know that decision trees are very easy to build and comprehend\n",
    "- they provide quick predictions\n",
    "- but, they do not perform great on unseen data \n",
    "- Random Forest is specifically here to combine the simplicity of the decision trees and reduce their inaccuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to go about with a random forest classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to follow the below steps:\n",
    "1. we must create several bootstrapped datasets. (this meand we need to randomly select different rows from our data into new datasets. These rows could also be repeated)\n",
    "2. Once we have the random bootstrapped datasets, we select random subset of the features or columns for each iteration instead of considering all the remaining columns.\n",
    "3. Repeat step 1 and 2, several times to build many trees\n",
    "4. Pass the new datapoint through all the built trees, and find individual results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post this, we can aggregate all the results and provide a common outcome from the algorithms. This step is called bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we measure how good our random forest is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we are randomly selecting rows for building the bootstrapped dataset, we end up with some rows not making the final dataset. These are called \"Out of bag dataset\"\n",
    "- Once we identify these out of bag samples, we can run them through all our trees and aggregate their results to check if the entire model is predicting accurately\n",
    "- Ultimately, the accuracy of our random forest depends on the proportion of out of bag samples correctly classified by our model\n",
    "- The proportion of incorrectly classified samples in out of bags dataset is called as out of bag error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to further imporve the random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we now about the \"out of bag error\", we can vary the number of randomly selected features we choose while building our random forest and find which has the lowest out of bag error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source:\n",
    "- https://www.youtube.com/watch?v=J4Wdy0Wc_xQ\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html#bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
