{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient boost is very similar to AdaBoost algorithm\n",
    "\n",
    "AdaBoost:\n",
    "- AdaBoost builds an initial stump and tries to predict the outcome. Based on the error made by the first stump subsequent stumps are created always focusing on building smaller learners.\n",
    "- After building multiple such stumps each having a weight, we can predict the outcome by performing a weighted average of predictions made by each stump.\n",
    "\n",
    "Gradient Boost:\n",
    "- In Gradient Boost, we start with a single leaf node as a predictor which represents the weigths of all the samples.\n",
    "- In general, the first value is always going to be an average in case of regression\n",
    "- after this step, the tree is built (larger than a stump but restricted) considering the errors made by the previous trees\n",
    "- The errors are computed using (observed - predicted) = psuedo residual\n",
    "- These pusedo residuals get computed for every entry\n",
    "- After computing the pseudo residuas, we build a tree using all our parameters which predicts the psuedo residual values\n",
    "- now to predict an output, we first predict it using the first tree and then using the second and add the two values\n",
    "        Eg: If first tree predicts 78, and second tree predicts 12.2, our final prediction would be 80.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: directly adding the two values might not be a very effective solution as we might end up with a tree which over fits our data\n",
    "- to solve such issues we introduce a learning_rate\n",
    "        Eg: old tree prediction + learning_rate* residual tree prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we get the new predicteed values we can again compute a new round of residuals and those can be used to build further trees\n",
    "#### NOTE: If multiple samples end up in the same leaf of the residual tree we can average out the values and put a single value in the leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the end of building the second residual tree we scale the prediction from both trees by the learning rate and add the results to the predictions of the original tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process continues until:\n",
    "1. we reach the maximum number of trees (or)\n",
    "2. new trees dont improve the residuals by much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boost for classification works in almost the same way as the Regression algorithm but there are a few subtle differences\n",
    "- In Regression case, we used to find the average of the results to get the data for our first leaf. In Classification we tend to calculate log(odds) (i.e. if 4 outputs are yes and 2 are no then log(odds) = log(4/2) = 0.7)\n",
    "- as we have a number for our leaf node, to determine the classification output we use logistic function.\n",
    "        probability of yes class = e^log_odds/(1+ e^log_odds)\n",
    "- Based on the probability value we can determine the output class by setting a threshold for classes (Eg: > 50% == yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the tree generated by above method we can ddetermine the predicted result for all the samples.\n",
    "- Using the predicted and original values we can determine psuedo residuals. Here if we assume there are 2 classes yes and no; there are 4 yes samples and 2 no samples; the predicted probability for all is 0.7; then the yes values can be given a value of 1; the no values can be given a value of 0; so 4*(1 - 0.7) + 2*(0.7 - 0) = 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source:\n",
    "- https://www.youtube.com/watch?v=3CC4N4z3GJc\n",
    "- https://www.youtube.com/watch?v=2xudPOBz-vs\n",
    "- https://www.youtube.com/watch?v=jxuNLH5dXCs\n",
    "- https://www.youtube.com/watch?v=StWY5QWMXCw\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
