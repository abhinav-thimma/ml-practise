{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is similar to Linear Regression, but unlike the name suggests it has more to do with classification than Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Linear Regression we try to fit a line through the data we have. Fitting this line can be done in many ways but for the best results we use metrics like square of residual sum. Changing these metrics gives us varients of Linear Regression like Lasso or Ridge Regression\n",
    "- In Logistic Regression (most simple case), we tend to perform classification based on the input data which could be continuous. Instead of fitting a line through our data we fit a log function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facts about Logistic function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log function ranges from 0 to 1 and is shaped like an S. Here the predictions we make are considered by their proximity to 0 or 1 to determine whether we predict class 0 or class 1"
   ]
  },
  {
   "attachments": {
    "logistic_regression.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logistic_regression.png](attachment:logistic_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As the above graph indicates, the logistic function which we fit to our data tries to predict the a value ranging between 0 and 1. \n",
    "- thise might seem like a continuous output and thus the name Regression. But, primarily logistic regression is used for classification tasks\n",
    "- Eg: Obese or not obese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which kind of data can  be used with logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression can work with continous data like (weight or height)\n",
    "- It can also work with categorical data (college grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we determine if a variable  is particularly useful in predicting the output?\n",
    "\n",
    "- We use wald's test for this. Refer wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We cannot simply use least sqaures to fit the logistic curve. So how does logictic regression give us a good curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of using the classic least squares, we tend to use Maximum likelihood (product of all the likelihood for data points given a certain line)\n",
    "- Here we try to fit the curve that maximizes the likelihood of the given points being classified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source:\n",
    "- https://www.youtube.com/watch?v=yIYKR4sgzI8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\ml-core\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.81781575e-01, 1.82184108e-02, 1.44190517e-08],\n",
       "       [9.71700038e-01, 2.82999314e-02, 3.01434635e-08]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
