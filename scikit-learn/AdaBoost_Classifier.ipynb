{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle of the AdaBoost algorithm is to combine several weak learners and make an aggregated prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is Ada boost different from a normal Decision Tree algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No depth Restirctions:\n",
    "- In a normal decision tree algorithm or even in the random forest algorithm we tend to build different trees with no restrictions on the depth of each tree.\n",
    "- In AdaBoost version of random forest, we tend to create trees with just one node and two leaves (also called as stump) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprised of weak learners:\n",
    "- As stumps have just a single node, it means that they can only classify the data using any one of the feature at any given time.\n",
    "- This makes them very weak learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unequal priority to stumps:\n",
    "- In a normal Random Forest implementation, we tend to give equal priority to the decision made by each tree and finally we aggregate our results\n",
    "- But, in AdaBoost implementation, we tend to bake in the concepts of variable weights right into the algorithm. This means that not all the stumps have an equal say in predicting the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stumps created before will influence the stumps created after:\n",
    "- In a Random forest implementation, each of the trees which is developed is built independantly.\n",
    "- In an Adaboost implementation, the error we get from the stump created earlier would influence the creation of future stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does AdaBoost work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Assign a weight to every every sample record we have. Initially all of them can be assigned a weight of 1/(no_of_samples)\n",
    "2. Find how each feature is independantly able to predict the outcome. Using the data we can compute the <b>weighted gini index</b> (weighted using the sample_weight) for each parameter and then choose the lowest one to be our first stump feature.\n",
    "3. Find the error for this stump by adding the weights of samples which were incorrectly classified by our model.\n",
    "4. Using the total error, we can determine the influence this stump has on the final result using the formula\n",
    "    \n",
    "    <b>Amount of say = (1/2)*log ((1 - total_error)/ total_error)</b>\n",
    "5. As our model classified a few samples wrongly, we can emphasise that the next stumps must focus on these specific samples by increasing their respective weights and decreasing the sample weight of correctly classified samples.\n",
    "    \n",
    "    <b>new_sample_weight for incorrect classification = old_weight* e^(amount_of_say)</b>\n",
    "    \n",
    "    Is the amount of say for the stump was larger we add a larger penalty in the form of sample_weight for negatively classified sample\n",
    "    \n",
    "    <b>new_sample_Weight for correct classifications = old_weight* e^ - (amount_of_say)</b>\n",
    "    \n",
    "    <b> NOTE: as the new weights total may exceed 1, we normalize each weight by diving by sum of all new weights</b>\n",
    "\n",
    "6. After the new weights are calculated we can use the existing samples or randomly pick samples (allowing duplication) from the existing samples to have same number of samples. This random picking of data is done by using the sample_weights as a distribution and picking values. As the higher weight samples would occupy more area under the distribution we have high chance that those samples would repeat themselves more.\n",
    "7. After the new data is formed we reset all the sample_weights to 1/(no_of_samples) and continue the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can a forest of stumps (created using AdaBoost) classify the samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we pass the new data to all the stumps\n",
    "- at the end we might get stumps which predict 0 (this can be any output) and the remaining which predict 1\n",
    "- we just add the amounts of say for each category of stumps and whichever has the higher amount of say we give out that result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the stumps created earlier influencing the one's to come?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as we are using the weighted gini function to determine which samples need to be focused on, we are indirectly influencing the stumps which are yet to come"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.youtube.com/watch?v=LsK-xG1cLYA\n",
    "- https://www.youtube.com/watch?v=NLRO1-jp5F8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666665"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
