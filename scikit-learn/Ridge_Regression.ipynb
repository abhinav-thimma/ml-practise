{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression and Least sqaures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we perform Linear Regression, we tend to fit a line to existing data which minimizes the squared error. \n",
    "- This schema works very well if our training and testing data are sufficiently large\n",
    "- But, if our training data is rather small, the act of trying to get the minimum possible error can sometimes lead to overfitting\n",
    "- To prevent this we use Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can Ridge Regression help with this overfitting of training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering a simple dataset where we have just a single parameter. this can be represeted by the equation:\n",
    "\n",
    "y = w + bx  where b is 1*1 value\n",
    "\n",
    "In Linear Regression,\n",
    "we tend to vary the values of b and w which minimize the (total squared residual)\n",
    "\n",
    "In Ridge Regression,\n",
    "we tend to vary the values of b and w which minimize the (total sqaured residual + lambda*slope^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here the term lambda*slope^2 would act as a bias source.\n",
    "- This bias term's use can be understood by a simple exmaple:\n",
    "    Assume we just have 2 points in our training set\n",
    "    - Then the Linear Regression would eventually give us a straight line passing through both these points.\n",
    "    - Consider the line equation is y = 3 + 2x\n",
    "    - In the same scenario Ridge Regression would give us a line which is close to both points but does not directly pass throguh them\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this slope based term add bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general if the slope of a line is large it means that small changes in the values x (inputs/ weights) will result in larger changes in the value y (output).\n",
    "\n",
    "Applying the same logic:\n",
    "- If our line is overfitting the data, it generally means that our line is more dependant on the chnages in the inputs\n",
    "- by introducing this extra positive term in the error calculation we tend to show preference for lines with less slope and thus less dependance on the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is lambda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:  lambda is a positive value (o <= lambda <= +infi)\n",
    "\n",
    "We can make similar insights about lamdba as we did for slope: \n",
    "- As we are adding a positve value to our final error score our eventual goal would be to reduce the error considering this additional term\n",
    "- Here, if we increase the lambda value from 1 to 2, then the bias we are introducing would further increase.\n",
    "- This means that with the increase in the lambda value, the slope of the final line would decrease\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we determine the best value of Lambda (herperparamter) ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way is to tune this as a hyperparameter:\n",
    "- we can use cross validation to find the best value of lambda for our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where can we use Ridge Regression?\n",
    "\n",
    "Ridge Rigression can be used for routine LinerRegression cases and also for Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source:  https://www.youtube.com/watch?v=Q81RR3yKn30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: alpha below is lambda\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients = [0.34545455 0.34545455], intercept = 0.13636363636363638\n"
     ]
    }
   ],
   "source": [
    "print(f'coefficients = {reg.coef_}, intercept = {reg.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the perfect lambda value for our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
       "       1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.01, coefficients = [0.47146402 0.47146402], intercept = 0.052357320099276905\n"
     ]
    }
   ],
   "source": [
    "print(f'alpha = {reg.alpha_}, coefficients = {reg.coef_}, intercept = {reg.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf\n",
    "- https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
