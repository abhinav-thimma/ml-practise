{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Voting classifier falls under the category of ensemble learning algorithms\n",
    "- The basic goal of ensemble learning is to build multiple models and generate a cumulative result from all of them\n",
    "- The idea being the cumulative answer from several not so good learners would be better than a single result from a very good learner\n",
    "- Ensembling techniques also help greatly reduce the overfitting of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two kinds of voting classifiers:\n",
    "    1. hard voting classifier\n",
    "    2. soft voting classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Voting classifier\n",
    "\n",
    "- If we choose hard voting, we are trying to find out which result was predicted by a majority of models involved in our ensemble learning\n",
    "- Eg: We have 4 sub models and they predicted [0, 0, 1, 0] our hard voting classifier would predict the outcome as 0\n",
    "- In such kind of voting, we look form a winner takes all system. The internal models will only give 1 result each and we choose the most frequent output\n",
    "- If there is a tie then the output class would be selected as the first value in ascending sequence of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft voting Classifier\n",
    "\n",
    "- In a soft votinng procedure, each model is responsible for giving out a confidence along with its output\n",
    "- Eg:\n",
    "    - M1:  1: 0.9,  0: 0.1\n",
    "    - M2:  1: 0.8,  0: 0.2\n",
    "    - M3:  1: 0.7,  0: 0.3\n",
    "    - M4:  1: 0.6,  0: 0.4\n",
    "- Our soft voting mechanism takes the average of each possible output as predicted by all sub models and which ever probability is higher that would be given as output\n",
    "- In above case, the average confidence would be,  1:  0.75,  0:  0.25, so our soft voting classifier would predict 1 as the output\n",
    "- Additionally we can provide weights for each model in both hard and soft voting cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Regressor:\n",
    "\n",
    "- The basic idea of a voting Regressor remains same as Voting Classifier\n",
    "- But we take the average of the predicted results of the regressors involved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source:\n",
    "- https://www.youtube.com/watch?v=dD7gvbfBiyA\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.04) [Logistic Regression]\n",
      "Accuracy: 0.94 (+/- 0.04) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.04) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='soft', weights=[2, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93 (+/- 0.07) [Decision Tree]\n",
      "Accuracy: 0.94 (+/- 0.04) [KNN]\n",
      "Accuracy: 0.95 (+/- 0.03) [SVC]\n",
      "Accuracy: 0.95 (+/- 0.05) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Decision Tree', 'KNN', 'SVC', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading some example data\n",
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifiers\n",
    "reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\n",
    "reg2 = RandomForestRegressor(random_state=1, n_estimators=10)\n",
    "reg3 = LinearRegression()\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "ereg = ereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
